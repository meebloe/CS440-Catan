# Catan AI Project (ML Agent with Unity Integration)

## Overview

This project implements an AI agent using a simple Multi-Layer Perceptron (MLP) neural network to play a 2-player version of the board game Settlers of Catan. The project features:

1.  **Unity Game Client (`client/CatanLearner.exe`):** Runs the Catan game logic, determines legal moves, serializes the game state to JSON, sends it to the AI server, receives the AI's chosen action, and executes it. Supports Human vs. Bot and Bot vs. Bot modes. Generates game logs for training.
2.  **Python AI Server (`server/catan_ai.py`):** A Flask server that receives game state JSON, vectorizes it (`server/game_state_encoder.py`), uses a trained PyTorch model (`server/model.py`) to predict the best action among the available legal moves (provided by the client), and returns the chosen action JSON.
3.  **Training Pipeline (`server/train_from_logs.py`):** A script to train the PyTorch model using game logs generated by the Unity client during self-play (`client/SelfPlayLogs`). Saves the trained model weights (`server/model_weights.pth`).
4.  **Launcher Script (`launch_bot_match.py`):** The main entry point to manage running the game and training the AI in different modes (`play`, `train`, `bulktrain`).

## Project Structure

.
├── README.md
├── client/ # Unity Game Client and related files
│ ├── CatanLearner.exe # The executable game client
│ └── ... (Other Unity files/folders)
├── server/ # Python AI Server, Model, and Training code
│ ├── catan_ai.py # Flask server handling API requests
│ ├── model.py # PyTorch MLP model definition
│ ├── game_state_encoder.py # Converts game state JSON to a numerical vector
│ ├── action_mapping.py # Maps action descriptions to numerical indices
│ ├── train_from_logs.py # Script to train the model from game logs
│ ├── model_weights.pth # Saved weights for the trained model
│ ├── iterations/ # Checkpoints saved during bulk training
│ └── ... (init.py)
├── launch_bot_match.py # Main script to launch game/server and manage modes
├── requirements.txt # Python dependencies
└── ... (Other potential test/output files)


## Prerequisites

*   **Conda:** For Python environment management. Download and install Anaconda or Miniconda.
*   **Python:** Version 3.9+ recommended.
*   **Unity Game Build:** The `client/CatanLearner.exe` built from the Unity project (provided here).

## Setup

1.  **Clone the Repository:**
    ```bash
    git clone <your-repository-url>
    cd catan_ai_project
    ```

2.  **Create and Activate Conda Environment:**
    Open your terminal or Anaconda Prompt.
    ```bash
    # Create the environment (use python>=3.9)
    conda create --name catan_ai_env python=3.13.2

    # Activate the environment
    conda activate catan_ai_env
    ```
    *(Your terminal prompt should change to show `(catan_ai_env)`)*

3.  **Install Dependencies:**
    While the environment is active, install the required Python packages using `requirements.txt`:
    ```bash
    pip install -r requirements.txt
    ```
    *(This will install Flask, PyTorch, NumPy, requests etc...)*

## How to Run

Use the `launch_bot_match.py` script from the project root directory while your `catan_ai_env` conda environment is active.

**Important:** Make sure you are in the `catan_ai_env` environment before running any commands.

1.  **Play Mode (Human vs. AI):**
    *   Launches the AI server.
    *   Launches the Unity game configured for a human player (Player 0) against the AI bot (Player 1).
    *   After the game finishes, it runs a quick training pass on the log generated from that single game (using a higher learning rate).
    ```bash
    conda activate catan_ai_env
    python launch_bot_match.py play
    ```

2.  **Training Mode (Bot vs. Bot - Multiple Games):**
    *   Clears previous logs (moves them to `client/OldLogs/`).
    *   Launches the AI server in `TRAIN` mode (enables exploration during inference).
    *   Launches the specified number (`n`) of Unity games concurrently, configured for Bot vs. Bot self-play.
    *   Waits for all games to complete.
    *   Runs the training script (`train_from_logs.py`) on *all* logs generated in the `client/SelfPlayLogs/` directory from this batch of games (using a normal learning rate).
    *   Saves the updated `model_weights.pth`.
    ```bash
    conda activate catan_ai_env
    python launch_bot_match.py train <n>
    ```
    *(Replace `<n>` with the number of games to play, e.g., `python launch_bot_match.py train 5`)*

3.  **Bulk Training Mode (Iterative Bot vs. Bot):**
    *   Designed for longer unsupervised training cycles.
    *   Clears previous logs.
    *   Launches the AI server.
    *   **Repeats `y` times:**
        *   Runs `x` concurrent Bot vs. Bot games.
        *   Waits for the `x` games to finish.
        *   Trains the model on the logs from those `x` games.
        *   Saves a checkpoint of the model weights to the `server/iterations/` folder (e.g., `settlerbot_0.pth`, `settlerbot_1.pth`, ...).
        *   Clears the logs before the next set.
    ```bash
    conda activate catan_ai_env
    python launch_bot_match.py bulktrain <x> <y>
    ```
    *(Replace `<x>` with the number of games per set, and `<y>` with the number of sets/iterations. E.g., `python launch_bot_match.py bulktrain 10 5` runs 10 games, trains, checkpoints, clears logs, and repeats this 5 times.)*

## Training Process

*   **Log Generation:** When the Unity client runs in Bot vs. Bot mode (`train` or `bulktrain`), it saves detailed logs of each game turn (state, action taken, reward) as `.jsonl` files in `client/SelfPlayLogs/`.
*   **Training Script:** `server/train_from_logs.py` reads all `.jsonl` files in the log directory.
*   **Data Conversion:** For each recorded turn, it vectorizes the `state` using `game_state_encoder.py` and gets the numerical index of the `action` using `action_mapping.py`. It also uses the recorded `reward` (potentially adding a bonus for winning moves).
*   **Model Update:** It trains the `CatanSimpleMLP` model defined in `server/model.py` using the collected (state_vector, action_index, reward) tuples. It uses an MSELoss function, treating it somewhat like a Q-learning update where the target for the taken action is the observed reward.
*   **Weight Saving:** After training epochs, the updated model weights are saved to `server/model_weights.pth`. In `bulktrain` mode, timestamped or numbered checkpoints are also saved in `server/iterations/`.
*   **Log Clearing:** The `launch_bot_match.py` script moves processed logs from `SelfPlayLogs` to `OldLogs` after training to prevent re-training on the same data in subsequent runs.

## JSON Communication Format

Communication between the game client (Unity) and the AI server (Python Flask) uses JSON objects sent over HTTP POST requests to the `/get_action` endpoint.

*(The JSON structure description from your original README is detailed and accurate. You can keep it here as it was, or potentially move it to a separate `API_DOCUMENTATION.md` if the README becomes too long.)*

### 1. State JSON (Sent from Game Client to AI Server)

This JSON object represents the complete snapshot of the game state needed for the AI to make a decision.

**Structure:**

```javascript
{
  "gameStateId": "string (optional)", // Unique identifier for the state (for logging/debugging)
  "currentPlayerIndex": integer,      // Index (0 or 1) of the player whose turn it is
  // "diceRolled": boolean,           // Note: Check if these are still used/needed by the encoder/model
  "diceResult": integer, // Changed based on encoder: single int (2-12, 0 if none)
  // "mustMoveRobber": boolean,         // Note: Check if these are still used/needed by the encoder/model
  // "robberHexIndex": integer,         // Note: Check if these are still used/needed by the encoder/model
  "hexes": [                          // List of 19 hex objects (MUST be in consistent order)
    {
      "id": integer,                  // Hex index (0-18)
      "resource": "string" | null,    // Resource type ("LUMBER", "BRICK", "WOOL", "GRAIN", "ORE", "DESERT") or null
      "numberToken": integer | null   // Dice number (2-12, excluding 7) or null (for desert)
    },
    // ... 18 more hex objects
  ],
  "roads": [                          // List of 72 potential road edge objects (MUST be in consistent order)
    {
      "id": integer,                  // Edge index (0-71)
      "ownerPlayerIndex": integer     // Owning player index (0 or 1), or -1 if unoccupied
    },
    // ... 71 more road objects
  ],
  "buildings": [                      // List of 54 potential building intersection objects (MUST be in consistent order)
    {
      "id": integer,                  // Intersection index (0-53)
      "ownerPlayerIndex": integer,    // Owning player index (0 or 1), or -1 if unoccupied
      "type": "string"                // Type of building ("NONE", "SETTLEMENT", "CITY")
    },
    // ... 53 more building objects
  ],
  "players": [                        // List of 2 player objects
    {
      "index": integer,               // Player index (0 or 1)
      "resources": {                  // Dictionary of resource counts
        "LUMBER": integer,
        "BRICK": integer,
        "WOOL": integer,
        "GRAIN": integer,
        "ORE": integer
      },
      "victoryPoints": integer        // Current public victory point count
    },
    { // Player 1 object structure is identical
      "index": 1,
      "resources": { /* ... */ },
      "victoryPoints": integer
    }
  ],
  "availableActions": [               // *** CRITICAL LIST *** of all LEGAL actions the current player can take NOW
    // Each object in this list represents one possible, valid action.
    // Format depends on action type (must match action_mapping.py):
    {"actionType": "BUILD_ROAD", "edgeIndex": integer},
    {"actionType": "BUILD_SETTLEMENT", "intersectionIndex": integer},
    {"actionType": "BUILD_CITY", "intersectionIndex": integer}, // Index of the settlement to upgrade
    {"actionType": "BANK_TRADE_4_1", "resourceOut": "string", "resourceIn": "string"}, // Resource names
    // {"actionType": "MOVE_ROBBER", ...}, // Note: Add robber action if implemented & mapped
    {"actionType": "END_TURN"}
    // ... potentially other actions if features are added and mapped in action_mapping.py
  ]
}